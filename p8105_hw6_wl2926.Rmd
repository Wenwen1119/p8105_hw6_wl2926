---
title: "Homework 6 "
author: "Wenwen Li"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(broom)
library(purrr)
library(corrplot)
library(gridExtra)
library(caret)
library(modelr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```
### Problem 1

In the data cleaning code below we create a `city_state` variable, change `victim_age` to numeric, modifiy victim_race to have categories white and non-white, with white as the reference category, and create a `resolution` variable indicating whether the homicide is solved. Lastly, we filtered out the following cities: Tulsa, AL; Dallas, TX; Phoenix, AZ; and Kansas City, MO; and we retained only the variables `city_state`, `resolution`, `victim_age`, `victim_sex`, and `victim_race`.

```{r q1_data_cleaning}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) |> 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) |> 
  filter(victim_race %in% c("White", "Black")) |> 
  filter(!(city_state %in% c("Tulsa, AL", "Dallas, TX", "Phoenix, AZ", "Kansas City, MO"))) |> 
  select(city_state, resolution, victim_age, victim_sex, victim_race)
```

Next we fit a logistic regression model using only data from Baltimore, MD. We model `resolved` as the outcome and `victim_age`, `victim_sex`, and `victim_race` as predictors. We save the output as `baltimore_glm` so that we can apply `broom::tidy` to this object and obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing non-white victims to white victims.

```{r q1_glm_baltimore}
baltimore_glm = 
  filter(homicide_df, city_state == "Baltimore, MD") |> 
  glm(resolution ~ victim_age + victim_sex + victim_race, family = binomial(), data = _)

baltimore_glm |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(OR, OR_CI_lower, OR_CI_upper) |>
  knitr::kable(digits = 3)
```

Below, by incorporating `nest()`, `map()`, and `unnest()` into the preceding Baltimore-specific code, we fit a model for each of the cities, and extract the adjusted odds ratio (and CI) for solving homicides comparing non-white victims to white victims. We show the first 5 rows of the resulting dataframe of model results.

```{r q1_glm_all_cities}
model_results = 
  homicide_df |> 
  nest(data = -city_state) |> 
  mutate(
    models = map(data, \(df) glm(resolution ~ victim_age + victim_sex + victim_race, 
                             family = binomial(), data = df)),
    tidy_models = map(models, broom::tidy)) |> 
  select(-models, -data) |> 
  unnest(cols = tidy_models) |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(city_state, OR, OR_CI_lower, OR_CI_upper)

model_results |>
  slice(1:5) |> 
  knitr::kable(digits = 3)
```

Below we generate a plot of the estimated ORs and CIs for each city, ordered by magnitude of the OR from smallest to largest. From this plot we see that most cities have odds ratios that are smaller than 1, suggesting that crimes with male victims have smaller odds of resolution compared to crimes with female victims after adjusting for victim age and race. This disparity is strongest in New yrok. In roughly half of these cities, confidence intervals are narrow and do not contain 1, suggesting a significant difference in resolution rates by sex after adjustment for victim age and race. 

```{r q1_plot}
model_results |> 
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = OR_CI_lower, ymax = OR_CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
### Problem 2
## Accessing the data
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

```{r}
#function to calculate R-squared and log(beta1 * beta2)
calculate_metrics = function(data) {
  model = lm(tmax ~ tmin + prcp, data = data)
  tidy_results = broom::tidy(model)
  r_squared = broom::glance(model)$r.squared
  #taking the logarithm after converting to absolute values
  beta_product = log(tidy_results$estimate[2]*
                        tidy_results$estimate[3])
  metrics_df = data.frame(R_squared = r_squared,
                           Beta_product = beta_product)
  return(metrics_df)
}

calculate_metrics(weather_df)

```

```{r}
#function to extract R-squared and compute log(beta1*beta2) for each bootstrap replicate
calculate_metrics = function(data) {
  data= data.frame(data)
  model = lm(tmax ~ tmin + prcp, data = data)
  tidy_results = broom::tidy(model)
  r_squared = broom::glance(model)$r.squared
  #taking the logarithm of beta1*beta2
  beta_product = log(tidy_results$estimate[2] * tidy_results$estimate[3])
  #return a data frame for the current replicate
  data.frame(R_squared = r_squared, Beta_product = beta_product)
}

#5000 bootstraps using modelr's bootstrap function
boot = modelr::bootstrap(weather_df, 5000)

#use the custom tidying function
models = map(boot$strap, calculate_metrics)

#combine the results into a data frame
bootstrap_results = bind_rows(models, .id = "Replicate")

#view the results
head(bootstrap_results)
```

```{r}
#visualize bootstrap distribution of R-squared
ggplot(bootstrap_results, aes(x = R_squared)) +
  geom_histogram(binwidth = 0.005, alpha = 0.7) +
  labs(title = "Bootstrapped distribution of R-squared",
       x = "R-squared")
```

The bootstrap distribution of the R-squared metric from the 5000 bootstrap samples assumes a slight left-skewed distribution, from the shape of the histogram.  


```{r, warning=FALSE}
#visualize bootstrap distribution of log(beta1 * beta2)
ggplot(bootstrap_results, aes(x = Beta_product)) +
  geom_histogram(binwidth = 0.1, alpha = 0.7) +
  labs(title = "Bootstrapped distribution of log(beta1 * beta2)",
       x = "log(beta1 * beta2)")

```

The bootstrap distribution of the log(beta1 * beta2) value from the 5000 bootstrap samples indicates a heavy left-skewed distribution.   


```{r}
#compute 95% confidence intervals for R-squared and log(beta1*beta2)
confidence_intervals = summarize(bootstrap_results,
  CIlower_R2 = quantile(R_squared, 0.025),
  CIupper_R2 = quantile(R_squared, 0.975),
  CIlower_b1xb2 = quantile(Beta_product, 0.025, na.rm=TRUE),
  CIupper_b1xb2 = quantile(Beta_product, 0.975, na.rm=TRUE)) 

confidence_intervals

```

# Problem 3
## Data Importation and preparation

```{r}
#import birthweight data
birthweight_data = read.csv("data/birthweight.csv")
#display the structure of the dataset
str(birthweight_data)
```

```{r}
#check for missing data
sum(is.na(birthweight_data))
```
The baby birth weight dataset doesn't have missing values.    

```{r}
#convert variables to appropriate data dtypes
birthweight_data$babysex = factor(birthweight_data$babysex,
                                   levels = c(1, 2),
                                   labels = c("male", "female"))
birthweight_data$frace = factor(birthweight_data$frace,
                                 levels = c(1, 2, 3, 4, 8, 9), 
                                  labels = c("White", "Black", "Asian",
                                             "Puerto Rican", "Other",
                                             "Unknown"))
birthweight_data$mrace = factor(birthweight_data$mrace,
                                 levels = c(1, 2, 3, 4, 8, 9), 
                                  labels = c("White", "Black", "Asian",
                                             "Puerto Rican", "Other",
                                             "Unknown"))
birthweight_data$malform = factor(birthweight_data$malform,
                                   levels = c(0, 1),
                                   labels = c("absent", "present"))
birthweight_data$parity = as.factor(birthweight_data$parity)

#check the structure of the dataset after conversion
str(birthweight_data)
```
## Data driven model building.      

The response variable in this study is the 'bwt', which indicates a new born babyâ€™s weight in grams.    

To decide on the best features to use in a predictive model, we first explore the statistical properties of all numerical and categorical variables.    

```{r}
#descriptive statistics of birthweight data
summary(birthweight_data)
```

The descriptive statistics depict an almost equal number of babies from both sexes, with male babies having the slightly higher number.     
Some variables such as `pnumlbw` and `pnumsga` appear to have constant values. 

### Histograms of all numerical variables.     
```{r, message=FALSE}
#select only numerical variables
numerical_vars = birthweight_data[, sapply(birthweight_data,
                                            is.numeric)]
#plot histograms for all numerical variables
plot_histograms = function(dataset) {
  #get the number of rows and columns needed for the plots
  ncol = 4
  nrow = 4

  #create a histogram for each variable and store in a list
  hist_list = lapply(names(dataset), function(x) {
    ggplot(dataset, aes(x = .data[[x]])) +
      geom_histogram() + 
      labs(y="Frequency",
           x= x)+
      theme(axis.title = element_text(size = 7),
            axis.text.x = element_text(hjust = 1))
  })

  #visualize the histograms using grid layout
  grid.arrange(grobs = hist_list, ncol = ncol, nrow = nrow)
}
plot_histograms(numerical_vars)
```
- From the histograms, only the `smoken` appears significantly skewed (to the right). A log transformation was applied to reduce the skewness and the results re-visualized. The histograms of the `pnumlbw` and `pnumsga` confirm the suspicions for zero-variance variables.     


```{r}
#log transform smoken; make a small addition to avoid infinity values
birthweight_data$smoken = log(birthweight_data$smoken+1e5)
par(mfrow=c(1,2))
hist(numerical_vars$smoken,
     xlab = "smoken",
     main="Un-transformed 'smoken'")
hist(birthweight_data$smoken,
     xlab = "smoken",
     main="Log-transformed 'smoken'")
```
The histograms above show a comparison between the skewness in the un-transformed and log-transformed `smoken` variable, with the latter showing a smaller spread and thus smaller effect of skewness in the modeling to be carried out.  
### Barplots of all categorical variables.      

```{r, message=FALSE}
#select only categorical variables
categ_vars = birthweight_data[, sapply(birthweight_data,
                                            is.factor)]
#plot barplots for all factor variables
plot_barplots = function(dataset) {
  #get the number of rows and columns needed for the plots
  ncol = 3
  nrow = 2

  #create a barplot for each variable and store in a list
  hist_list = lapply(names(dataset), function(x) {
    ggplot(dataset, aes(x = .data[[x]])) +
      geom_bar() + 
      labs(y="Frequency",
           x= x)+
      coord_flip()+
      theme(axis.title = element_text(size = 7),
            axis.text.x = element_text(hjust = 1))
  })

  #visualize the barplots using grid layout
  grid.arrange(grobs = hist_list, ncol = ncol, nrow = nrow)
}
plot_barplots(categ_vars)
```
Some categorical variables, `malform` and `parity` in particular, have heavy class imbalances, and may thus contribute little to no explnatory power of the variation in birthweight of babies.


### Correlation analysis    

```{r}
#calculate the correlation matrix
cor_matrix = cor(numerical_vars, method = "pearson")

#create a heatmap for correlation visualization
corrplot(cor_matrix, method = "color",
         type = "upper", tl.col = "black",
         tl.srt = 45, hclust.method="complete")

```
The response variable `bwt` indicates weak to moderate positive and negative correlations with all the numerical variables. Notably, the response variable has a weak negative correlation with the `smoken` variable. The `ppbmi`, `ppwt`, and the `delwt` variables appear to have high correlations. They are all descriptive of a mother's weight for either before or after delivery. It might not be necessary to include all of them. For instance, the `ppbmi` is a direct derivative of the `ppwt`, and therefore the two are expected to be positively correlated.      
### Near zero variance/ Zero variance variables.    

Zero-variance variables are features in a dataset that do not add explanatory power to the observed variation in a dataset with respect to the response variable.   

```{r}
nzv = nearZeroVar(birthweight_data, saveMetrics= TRUE)
#number of variables with near zero variance
nzv |> filter(nzv == TRUE) |> nrow()
#removing near zero variance variables
birthweight_data = birthweight_data [, !nzv$nzv]
```
        
Four variables were found to have zero or near zero variance, and were excluded from the data set. These included the `malform`, `parity`, `pnulbw`, and `pnumsga`. 
### Multicollinearity.      

Highly correlated variables could cause multicollinearity, which could make it difficult to estimate the effects of individual features on the response variable.    

## Multiple linear regression modeling.      

A total of 14 variables, including; babysex, bhead, blength, delwt, fincome, frace, gaweeks, menarche, mheight, momage, mrace, ppwt, and smoken were selected for the linear model building. 
       
```{r}
#multiple linear regression model
lm_model = lm(bwt ~ babysex + bhead + blength + delwt + fincome + frace +
                 gaweeks + menarche + mheight + momage + mrace +
                 ppwt + smoken, data = birthweight_data)
#display a summary of the model
summary(lm_model)
```
       ### Fitted vs. Residual plot.      

```{r,message=FALSE}
#add predictions from the linear model
preds_resid_df = add_predictions(birthweight_data, lm_model, var = "pred")
#add residuals from the lienar model
preds_resid_df = add_residuals(preds_resid_df, lm_model, var="resid")


ggplot(preds_resid_df, aes(x = pred, y = resid)) +
  geom_point(color = "brown", size = 3, alpha=0.5) +
  geom_smooth(method = "lm", color = "blue") +
  labs(title = "Fitted birthweight values vs Residuals",
       x = "Fitted birthweight", 
       y = "Residuals")

```
        
There isn't a clear pattern, trend, or shape in the plot of the fitted values versus the residuals of the multiple linear regression model. The residuals from the fitted linear model do not depict a significant deviation from the expectation of homoscedasticity which is; the spread or dispersion of residuals should be relatively constant across all levels of fitted values.  
There isn't a clear pattern, trend, or shape in the plot of the fitted values versus the residuals of the multiple linear regression model. The residuals from the fitted linear model do not depict a significant deviation from the expectation of homoscedasticity which is; the spread or dispersion of residuals should be relatively constant across all levels of fitted values.    


## Model comparison.     


```{r}
#alternative model 1 using length at birth and gestational age as predictors 
model1 = lm(bwt ~ blength + gaweeks, data = birthweight_data)
summary(model1)
#alternative model 2 using head circumference, length, sex, and all their interactions
model2 = lm(bwt ~ bhead * blength * babysex, data = birthweight_data)
summary(model2)
```

        
The alternative models have lower adjusted R-squared values compared to the proposed linear model.     


```{r}
#list of models
models = list(lm_model, model1, model2)

#function to calculate prediction errors for a single model
calculate_errors = function(model, train_data, test_data) {
  #add predictions to the test set
  predictions_df = add_predictions(test_data, model, var = "pred")
  #add residuals to the predictions
  predictions_df = add_residuals(predictions_df, model, var = "resid")
  #calculate RMSE (Root Mean Squared Error)
  rmse = sqrt(mean(predictions_df$resid^2))
  return(rmse)
}

#function to perform 1000 cross-validations and calculate errors for each model
crossval_model = function(model) {
  #create a cross-validation object
  cv_results = crossv_mc(birthweight_data, n = 1000)
  #calculate prediction errors for each fold
  errors = map2_dbl(cv_results$train, cv_results$test, calculate_errors, model = model)
  #return the errors
  return(errors)
}

#apply the cross-validation function to each model
errors_list = map(models, crossval_model)

#model descriptions
model_names = c("proposed_model","main_effects","three_way_interaction")

#create a data frame with model names as columns
errors_df = as.data.frame(setNames(errors_list, model_names))

#view the resulting data frame
head(errors_df)
```


### Mean Cross validated RMSE.    

```{r}
#mean CV error Comparison
errors_df |>
  summarise_all(mean)
```
         
Proposed model achieves the lowest cross validated error of the three models. The three way interaction model using head circumference, length, and sex of the babies is the second best predictive model even though it uses just 3 variables (and their interactions) of the 14 applied in the proposed model.     


### Boxplots of cross-validated errors.     

```{r}
#boxplot of Errors
errors_df |>
  gather(key = "Model", value = "Error") |>
  ggplot(aes(x = Model, y = Error)) +
  geom_boxplot() +
  labs(title = "Boxplots of CV errors", x = "Model", y = "Error")

```
          
All the three models show outliers in the cross-validated errors.     


### Histograms of CV errors.    

```{r}
#Cross-Validation Error distribution
errors_df |>
  gather(key = "Model", value = "Error") |>
  ggplot(aes(x = Error)) +
  facet_wrap(~Model)+
  geom_histogram(binwidth = 10, fill = "skyblue", color = "black") +
  labs(title = "Cross-Validation Error Distribution", x = "Error", y = "Frequency")

```

        
The CV error histograms ofthe models show deviations from the normal distribution.